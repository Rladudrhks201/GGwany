{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.11.0 tokenizers-0.13.2 transformers-4.24.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script huggingface-cli.exe is installed in 'C:\\Users\\user\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script transformers-cli.exe is installed in 'C:\\Users\\user\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕', '내', '이름', '##은', '김영', '##관', '##이', '##야']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ElectraTokenizer\n",
    "\n",
    "tokenizer = ElectraTokenizer.from_pretrained('monologg/koelectra-base-v3-discriminator')\n",
    "tokenizer.tokenize('안녕 내 이름은 김영관이야')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 11655, 4279, 8553, 3], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('안녕하세요')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', ',', '월요일', '##부터', '공부', '##를', '하네', '##요']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('빩뷁녋, 월요일부터 공부를 하네요')\n",
    "# 정상적이지 않은 단어는 UNK로 표기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size\n",
    "# 저장된 단어의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 16, 10428, 6406, 7079, 4110, 21347, 4150]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(['[UNK]', ',', '월요일', '##부터', '공부', '##를', '하네', '##요'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', ',', '월요일', '##부터', '공부', '##를', '하네', '[UNK]']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([1, 16, 10428, 6406, 7079, 4110, 21347, 41500])\n",
    "# id를 다시 단어로 표기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토크나이저 훈련\n",
    "\n",
    "1. 어떤 기법을 사용할지\n",
    "2. vocab_size 결정 >> 작으면 메모리 이득, 크면 의미를 잘 표현할 수 있다.\n",
    "3. corpus >> 통계적으로 분석, vocab\n",
    "\n",
    "##### tokenizer vocab에 수동으로 추가하는 방법도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e03a4790794987942fb4dd9002ada2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dadb99ef198b4becae214c5e4bf12e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c8a73ef7e849c587ca0c8eca7541f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ᄋ',\n",
       " '##ᅡ',\n",
       " '##ᆫ',\n",
       " '##ᄂ',\n",
       " '##ᅧ',\n",
       " '##ᆼ',\n",
       " '##ᄒ',\n",
       " '##ᅡ',\n",
       " '##ᄉ',\n",
       " '##ᅦ',\n",
       " '##ᄋ',\n",
       " '##ᅭ']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('안녕하세요')\n",
    "# 영어처리 모델이라 잘 되지 않는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2a4e435fce40dbbbe33d28afa9de67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81716f456bf4d499c7310125755ed4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b700ee76734228b755a50a943ba977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multi_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안', '##녕', '##하', '##세', '##요', '?']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_tokenizer.tokenize('안녕하세요?')\n",
    "# multi 모델에는 '안녕'이라는 단어가 vocab에 포함되어 있지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9fbe0e02c14542937fd09ec1725502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4fa72e7cc11458a84c30a64d23849cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01982b679115465d9382323ad53adaa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ġ-', 'Ġ\"', 'Ġhave']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([111,22,33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28348"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids('grave')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert모델\n",
    "\n",
    "input = '--- [MASK] ---'\n",
    "\n",
    ">> 모델 적용 >>\n",
    "\n",
    "output = '마스킹하기 전의 원래 문장'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3가지 종류 언어모델**\n",
    ">> 언어에 대한 통계적 이해를 가진 모델\n",
    "\n",
    "1. Decoder-only 모델 Auto regressive model (GPT1,GPT2,GPT3)\n",
    "    - vectorize 된 input을 새로운 text로 decode한다\n",
    "\n",
    "    - few shot learning 사용 (힌트의 개수에 따라 one,few,..)\n",
    "    - 단점: 모델이 너무 큼\n",
    "\n",
    "2. Encoder-only 모델 Auto encoder (Bert,RoBERTa,Electra,albert,...)\n",
    "    - input을 vectorize 시킨다\n",
    "    - 자연어처리에서 의미는 vector와 관련이 있다\n",
    "\n",
    "    - pre-training (비지도학습) >> fine-tuning (추가 훈련, 지도학습)\n",
    "    - 라벨링이 되지 않은 데이터를 사전학습 -> 추가 훈련\n",
    "    - Downstream task (번역,문장 분류,질의응답,...)\n",
    "    - 이유?\n",
    "        1. 성능이 기존 방식보다 훨씬 좋다\n",
    "        2. 소량의 데이터로도 학습이 훨씬 빨리 된다\n",
    "    - CLS 토큰을 문장앞에 넣어 문장을 대표하는 벡터임을 알려줌\n",
    "    - 각 단어를 나타내는 토큰의 평균을 문장을 보여주는 형식으로 할 수도 있음, 성능 좋은 쪽을 선택\n",
    "    \n",
    "\n",
    "3. Encoder-Decoder 모델 (BART,T5,...)\n",
    "\n",
    "    - Transformer 모델과 구조는 동일하다\n",
    "    - Pre-training >> fine tuning\n",
    "    - 생성관련 Task 사용\n",
    "    \n",
    "\n",
    "nlp\n",
    "훈련\n",
    "배치 사이즈 클수록 성능이 좋아지는 경향이 있음 (gpu 성능이 중요한 이유중 하나)\n",
    "\n",
    "KOnlp pretained model\n",
    "\n",
    "1. koelectra\n",
    "2. klue-roberta\n",
    "3. koBERT\n",
    "\n",
    "\n",
    "embedding?\n",
    "고차원의 자료를 저차원의 자료로 변경하면서 필요한 정보를 보존하는 것을 embedding이라고 함\n",
    "\n",
    "attention?\n",
    "Decoder에서 추론 시점에 Encoder 토큰을 얼마나 참조하냐?\n",
    "\n",
    "self-attention?\n",
    "Encoder에서 자기 자신에게도 가중치를 부여\n",
    "\n",
    "BERT 모델 <- Transformer 모델에서 decoder을 제거한 모델\n",
    "\n",
    "```\n",
    "질의 응답\n",
    "\n",
    "[CLS] 포메라니안은 어떤 종이야? [SEP] 포메라니안은 독일 태생의 사냥견으로, 노는 것을 좋아한다. [SEP]\n",
    "\n",
    "정답: 노는 것을\n",
    "```\n",
    "\n",
    "\n",
    "    -Encoder 모델은 생성,task 불가능, 추출은 가능\n",
    "    -Decoder 모델은 가능\n",
    "\n",
    "    - chatbot을 Encoder모델로 생성하려면?\n",
    "    >> (답변1,2,3,4)\n",
    "    질문 > 뽑아낸다\n",
    "\n",
    "    - chatbot을 Decoder모델로 생성하려면?\n",
    "    >> 좀 더 사람같은 느낌, 위험성, 엉뚱한 대답이 나올 때가 있음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text to Image 모델\n",
    "\n",
    "- BERT >> 문장을 벡터로 만들어준다\n",
    "\n",
    "- 벡터를 이미지로 만든다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
